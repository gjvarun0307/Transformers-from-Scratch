{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe292cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/trans/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from config import get_config, get_weights_file_path\n",
    "from train import get_model, get_validation, get_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5dfc736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n",
      "Max length of src sentence: 309\n",
      "Max length of trg sentence: 274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device:\", device)\n",
    "\n",
    "config = get_config()\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_trg = get_ds(config)\n",
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_trg.get_vocab_size()).to(device)\n",
    "\n",
    "# load pretrained model\n",
    "model_path = \"/Users/varun/Hecker/Playground/Project/Transformers from Scratch/models/opus-en-it/tmodel_19\"\n",
    "state = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state[\"model_save_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27beff3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "--------------------------------------------------------------------------------\n",
      "Source Text: ['Georgiana sat on a high stool, dressing her hair at the glass, and interweaving her curls with artificial flowers and faded feathers, of which she had found a store in a drawer in the attic.']\n",
      "Expected Text: ['Georgiana era seduta su una sedia alta davanti allo specchio e intrecciava fiori artificiali e penne sbiadite, trovate in soffitta, ai suoi capelli.']\n",
      "predicted Text: ['tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone']\n",
      "torch.Size([1, 1])\n",
      "--------------------------------------------------------------------------------\n",
      "Source Text: ['Georgiana sat on a high stool, dressing her hair at the glass, and interweaving her curls with artificial flowers and faded feathers, of which she had found a store in a drawer in the attic.', '\"With pleasure,\" I replied; and I felt a thrill of artist-delight at the idea of copying from so perfect and radiant a model.']\n",
      "Expected Text: ['Georgiana era seduta su una sedia alta davanti allo specchio e intrecciava fiori artificiali e penne sbiadite, trovate in soffitta, ai suoi capelli.', '— Certo, — risposi. Era un piacere per me di avere un così bel modello.']\n",
      "predicted Text: ['tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone', 'volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute']\n",
      "torch.Size([1, 1])\n",
      "--------------------------------------------------------------------------------\n",
      "Source Text: ['Georgiana sat on a high stool, dressing her hair at the glass, and interweaving her curls with artificial flowers and faded feathers, of which she had found a store in a drawer in the attic.', '\"With pleasure,\" I replied; and I felt a thrill of artist-delight at the idea of copying from so perfect and radiant a model.', 'That is natural.']\n",
      "Expected Text: ['Georgiana era seduta su una sedia alta davanti allo specchio e intrecciava fiori artificiali e penne sbiadite, trovate in soffitta, ai suoi capelli.', '— Certo, — risposi. Era un piacere per me di avere un così bel modello.', 'E questo è comprensibile.']\n",
      "predicted Text: ['tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone', 'volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute', 'giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere']\n",
      "torch.Size([1, 1])\n",
      "--------------------------------------------------------------------------------\n",
      "Source Text: ['Georgiana sat on a high stool, dressing her hair at the glass, and interweaving her curls with artificial flowers and faded feathers, of which she had found a store in a drawer in the attic.', '\"With pleasure,\" I replied; and I felt a thrill of artist-delight at the idea of copying from so perfect and radiant a model.', 'That is natural.', \"You must be feeling envious.'\"]\n",
      "Expected Text: ['Georgiana era seduta su una sedia alta davanti allo specchio e intrecciava fiori artificiali e penne sbiadite, trovate in soffitta, ai suoi capelli.', '— Certo, — risposi. Era un piacere per me di avere un così bel modello.', 'E questo è comprensibile.', 'Lo invidiate, penso.']\n",
      "predicted Text: ['tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone tallone', 'volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute volute', 'giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere', 'giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere giornaliere']\n",
      "torch.Size([1, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_trg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mseq_len\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hecker/Playground/Project/Transformers from Scratch/train.py:65\u001b[39m, in \u001b[36mget_validation\u001b[39m\u001b[34m(model, validation_ds, tokenizer_src, tokenizer_trg, max_len, device, num_ex)\u001b[39m\n\u001b[32m     61\u001b[39m encoder_mask = batch[\u001b[33m'\u001b[39m\u001b[33mencoder_mask\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m encoder_input.size(\u001b[32m0\u001b[39m) == \u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBatch size for validation should be 1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m model_out = \u001b[43mgreedy_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_trg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m model_out_text = tokenizer_trg.decode(model_out.detach().cpu().numpy())\n\u001b[32m     68\u001b[39m source_text.append(batch[\u001b[33m'\u001b[39m\u001b[33msrc_text\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hecker/Playground/Project/Transformers from Scratch/train.py:34\u001b[39m, in \u001b[36mgreedy_decode\u001b[39m\u001b[34m(model, source, source_mask, tokenizer_src, tokenizer_trg, max_len, device)\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     33\u001b[39m decoder_mask = casual_mask(decoder_input.size(\u001b[32m1\u001b[39m)).type_as(source).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m decoder_output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m prob = model.project(decoder_output[:,-\u001b[32m1\u001b[39m])\n\u001b[32m     38\u001b[39m _, next_word = torch.max(prob, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hecker/Playground/Project/Transformers from Scratch/model.py:218\u001b[39m, in \u001b[36mTransformer.decode\u001b[39m\u001b[34m(self, encoder_output, src_mask, trg, trg_mask)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoder_output, src_mask, trg ,trg_mask):\n\u001b[32m    217\u001b[39m     trg = \u001b[38;5;28mself\u001b[39m.trg_pos(\u001b[38;5;28mself\u001b[39m.trg_embed(trg))\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/trans/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/trans/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hecker/Playground/Project/Transformers from Scratch/model.py:186\u001b[39m, in \u001b[36mDecoder.forward\u001b[39m\u001b[34m(self, x, encoder_output, src_mask, trg_mask)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, encoder_output, src_mask, trg_mask):\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m         x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.norm(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/trans/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/trans/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hecker/Playground/Project/Transformers from Scratch/model.py:171\u001b[39m, in \u001b[36mDecoderBlock.forward\u001b[39m\u001b[34m(self, x, encoder_output, src_mask, trg_mask)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, encoder_output, src_mask, trg_mask):\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresidual_connections\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmasked_self_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.residual_connections[\u001b[32m1\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m.cross_attention(x, encoder_output, encoder_output, src_mask))\n\u001b[32m    173\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.residual_connections[\u001b[32m2\u001b[39m](x, \u001b[38;5;28mself\u001b[39m.feed_forward)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/trans/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/trans/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hecker/Playground/Project/Transformers from Scratch/model.py:130\u001b[39m, in \u001b[36mResidualConnection.forward\u001b[39m\u001b[34m(self, x, sublayer)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, sublayer):\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.norm(x + \u001b[38;5;28mself\u001b[39m.dropout(\u001b[43msublayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hecker/Playground/Project/Transformers from Scratch/model.py:171\u001b[39m, in \u001b[36mDecoderBlock.forward.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, encoder_output, src_mask, trg_mask):\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.residual_connections[\u001b[32m0\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmasked_self_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    172\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.residual_connections[\u001b[32m1\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m.cross_attention(x, encoder_output, encoder_output, src_mask))\n\u001b[32m    173\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.residual_connections[\u001b[32m2\u001b[39m](x, \u001b[38;5;28mself\u001b[39m.feed_forward)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/trans/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/trans/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hecker/Playground/Project/Transformers from Scratch/model.py:104\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, q, k, v, mask)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, q, k, v, mask):\n\u001b[32m    102\u001b[39m     \u001b[38;5;66;03m# matrix multiplication of Q, K, V (batch_size, seq_len, d_model)\u001b[39;00m\n\u001b[32m    103\u001b[39m     query = \u001b[38;5;28mself\u001b[39m.w_q(q)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     key = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mw_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m     value = \u001b[38;5;28mself\u001b[39m.w_v(v)\n\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# split into h heads (batch_size, seq_len, h, d_k) --> (batch_size, h, seq_len, d_k)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/trans/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/trans/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/trans/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "get_validation(model, val_dataloader, tokenizer_src, tokenizer_trg, config['seq_len'], device, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dec76f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
